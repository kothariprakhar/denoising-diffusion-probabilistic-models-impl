{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Denoising Diffusion Probabilistic Models (DDPM)\n", "\n", "This notebook implements the core logic of the paper **'Denoising Diffusion Probabilistic Models' (Ho et al., 2020)**. We will use a simplified U-Net architecture and the **Fashion-MNIST** dataset to demonstrate the diffusion process efficiently in a Colab environment."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install matplotlib torch torchvision tqdm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Imports and Configuration\n", "We define the hyperparameters. Note that `timesteps` is set to 1000, matching the standard DDPM implementation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nimport os\n\nclass Config:\n    dataset_name = \"FashionMNIST\"\n    img_size = 28\n    channels = 1\n    timesteps = 1000\n    batch_size = 128\n    lr = 2e-4\n    epochs = 5  # Increase this for better quality (e.g., 20)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    save_dir = \"./ddpm_results\"\n\nconfig = Config()\nos.makedirs(config.save_dir, exist_ok=True)\nprint(f\"Using device: {config.device}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Diffusion Utilities\n", "This class handles the mathematics of the diffusion process:\n", "1. **Forward Process (q):** Adding noise based on a fixed Beta schedule.\n", "2. **Pre-calculations:** Computing alpha_cumprod to jump to any timestep $t$ in one step."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DiffusionUtils:\n    def __init__(self, timesteps, device):\n        self.timesteps = timesteps\n        self.device = device\n        \n        # Define beta schedule (linear)\n        self.beta_start = 0.0001\n        self.beta_end = 0.02\n        self.betas = torch.linspace(self.beta_start, self.beta_end, timesteps).to(device)\n        \n        # Pre-calculate alpha terms\n        self.alphas = 1. - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n        \n        # Calculations for diffusion q(x_t | x_0)\n        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n\n    def q_sample(self, x_0, t, noise=None):\n        \"\"\"Forward diffusion process: x_0 -> x_t\"\"\"\n        if noise is None:\n            noise = torch.randn_like(x_0)\n            \n        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_0.shape)\n        sqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_0.shape)\n        \n        return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n\n    def _extract(self, a, t, x_shape):\n        \"\"\"Extract coefficients at specified timesteps t and reshape to [batch, 1, 1, 1]\"\"\"\n        batch_size = t.shape[0]\n        out = a.gather(-1, t.cpu() if a.device.type == 'cpu' else t)\n        return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. The U-Net Model\n", "We implement a U-Net with Sinusoidal Time Embeddings. The time embedding allows the network to adapt its processing based on how noisy the image is."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class SinusoidalPositionEmbeddings(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, time):\n        device = time.device\n        half_dim = self.dim // 2\n        embeddings = np.log(10000) / (half_dim - 1)\n        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n        embeddings = time[:, None] * embeddings[None, :]\n        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n        return embeddings\n\nclass Block(nn.Module):\n    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n        super().__init__()\n        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n        if up:\n            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n        else:\n            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.bnorm1 = nn.BatchNorm2d(out_ch)\n        self.bnorm2 = nn.BatchNorm2d(out_ch)\n        self.relu  = nn.ReLU()\n\n    def forward(self, x, t):\n        # First Conv\n        h = self.bnorm1(self.relu(self.conv1(x)))\n        # Time Embedding Injection\n        time_emb = self.relu(self.time_mlp(t))\n        time_emb = time_emb[(..., ) + (None, ) * 2]\n        h = h + time_emb\n        # Second Conv\n        h = self.bnorm2(self.relu(self.conv2(h)))\n        return self.transform(h)\n\nclass SimpleUNet(nn.Module):\n    def __init__(self, img_channels=1, down_channels=[64, 128, 256], time_emb_dim=32):\n        super().__init__()\n        self.time_mlp = nn.Sequential(\n            SinusoidalPositionEmbeddings(time_emb_dim),\n            nn.Linear(time_emb_dim, time_emb_dim),\n            nn.ReLU()\n        )\n        \n        # Initial projection\n        self.conv0 = nn.Conv2d(img_channels, down_channels[0], 3, padding=1)\n\n        # Downsample\n        self.downs = nn.ModuleList([\n            Block(down_channels[i], down_channels[i+1], time_emb_dim)\n            for i in range(len(down_channels)-1)\n        ])\n        \n        # Upsample\n        self.ups = nn.ModuleList([\n            Block(down_channels[i+1], down_channels[i], time_emb_dim, up=True)\n            for i in range(len(down_channels)-1, 0, -1)\n        ])\n        \n        self.output = nn.Conv2d(down_channels[0], img_channels, 1)\n\n    def forward(self, x, timestep):\n        t = self.time_mlp(timestep)\n        x = self.conv0(x)\n        residuals = []\n        \n        # Down\n        for down in self.downs:\n            x = down(x, t)\n            residuals.append(x)\n            \n        # Up\n        for up in self.ups:\n            residual = residuals.pop()\n            x = torch.cat((x, residual), dim=1)\n            x = up(x, t)\n            \n        return self.output(x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Helper Functions: Loss & Sampling\n", "*   `get_loss`: Calculates MSE between predicted noise and actual noise.\n", "*   `sample`: Iteratively denoises pure noise starting from $T$ down to 0."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_loss(model, x_0, t, diffusion_utils):\n    noise = torch.randn_like(x_0)\n    x_t = diffusion_utils.q_sample(x_0, t, noise)\n    predicted_noise = model(x_t, t)\n    return F.mse_loss(noise, predicted_noise)\n\n@torch.no_grad()\ndef sample(model, diffusion_utils, image_size, batch_size=16, channels=3):\n    img = torch.randn((batch_size, channels, image_size, image_size), device=diffusion_utils.device)\n    \n    for i in tqdm(reversed(range(0, diffusion_utils.timesteps)), desc='Sampling', total=diffusion_utils.timesteps):\n        t = torch.full((batch_size,), i, device=diffusion_utils.device, dtype=torch.long)\n        predicted_noise = model(img, t)\n        \n        alpha = diffusion_utils.alphas[t][:, None, None, None]\n        alpha_hat = diffusion_utils.alphas_cumprod[t][:, None, None, None]\n        beta = diffusion_utils.betas[t][:, None, None, None]\n        \n        if i > 0:\n            noise = torch.randn_like(img)\n        else:\n            noise = torch.zeros_like(img)\n            \n        img = (1 / torch.sqrt(alpha)) * (img - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n        \n    img = (img.clamp(-1, 1) + 1) / 2\n    return img"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Training Loop\n", "We load the data and train the model."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Data Loader\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Lambda(lambda t: (t * 2) - 1)\n])\ndataset = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\ndataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n\n# Model Setup\nmodel = SimpleUNet(img_channels=config.channels).to(config.device)\noptimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\ndiffusion = DiffusionUtils(config.timesteps, config.device)\n\n# Training\nlosses = []\nprint(\"Starting Training...\")\n\nfor epoch in range(config.epochs):\n    model.train()\n    pbar = tqdm(dataloader)\n    epoch_loss = 0\n    for step, (images, _) in enumerate(pbar):\n        images = images.to(config.device)\n        t = torch.randint(0, config.timesteps, (images.shape[0],), device=config.device).long()\n        \n        loss = get_loss(model, images, t, diffusion)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        pbar.set_postfix(MSE=loss.item())\n    \n    avg_loss = epoch_loss / len(dataloader)\n    losses.append(avg_loss)\n    print(f\"Epoch {epoch+1}/{config.epochs} | Average Loss: {avg_loss:.4f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Visualization\n", "Finally, we plot the training loss and generate samples to check quality."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot Loss\nplt.figure(figsize=(10, 5))\nplt.plot(losses, label='Training Loss')\nplt.title('DDPM Training Loss over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Generate and Plot Samples\nprint(\"Sampling new images...\")\nmodel.eval()\nsamples = sample(model, diffusion, config.img_size, batch_size=32, channels=config.channels)\n\ngrid_img = make_grid(samples, nrow=8).cpu().permute(1, 2, 0)\nplt.figure(figsize=(12, 6))\nplt.imshow(grid_img)\nplt.title(\"Generated Fashion-MNIST Samples\")\nplt.axis('off')\nplt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Interpretation\n", "1. **Loss Curve:** Shows the convergence of the noise prediction task. A steady decline indicates the model is learning the structure of noise distribution relative to the data.\n", "2. **Generated Samples:** These images are generated purely from random noise. If successful, you should see clear Fashion-MNIST items (shirts, shoes, etc.)."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}